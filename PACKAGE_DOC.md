# Package Documentation

This file shows what users will see when viewing godoc for the pipeline package.

## Installation

```bash
go get hallenweaver/go-flow
```

## Quick Start

```go
import "hallenweaver/go-flow/pipeline"

// Create a pipeline with 4 workers
p := pipeline.New(4, func(ctx context.Context, x int) (int, error) {
    return x * 2, nil
})

// Start processing
jobs, results := p.Start(context.Background())

// Send jobs and receive results
go func() {
    defer close(jobs)
    jobs <- pipeline.Job[int]{ID: "1", Payload: 42}
}()

for result := range results {
    fmt.Printf("Result: %d\n", result.Value)
}
```

## Features

### ðŸš€ Type-Safe Generics
Process any input/output types without reflection or type assertions.

### ðŸ‘· Worker Pool Pattern
Fixed number of concurrent workers prevents resource exhaustion.

### â±ï¸ Per-Job Timeouts
Optional timeout for individual jobs using Go's context system.

### ðŸ“Š Built-in Metrics
Track job completion, failures, and cancellations with atomic counters.

### ðŸŽ¯ Context-Aware
Proper cancellation propagation and graceful shutdown.

### ðŸ”§ Configurable Buffers
Control memory usage and backpressure with channel buffering.

## Performance Characteristics

Based on benchmarks on Apple M4:

- **Throughput**: ~3.5M jobs/sec with 4 workers (minimal work)
- **Latency**: ~300ns per job (overhead only)
- **Memory**: Zero allocations for simple handlers
- **Timeout overhead**: ~600ns per job with timeout context
- **Worker scaling**: Near-linear scaling up to 4 workers

## Examples

See `example_test.go` for runnable examples:
- `ExampleNew` - Creating a pipeline
- `ExamplePipeline_Start` - Using Start() method
- `ExamplePipeline_Run` - Using Run() with existing channels
- `ExampleJob_timeout` - Per-job timeouts
- `ExamplePipeline_Stats` - Collecting statistics
- `ExampleOptions` - Configuring buffer sizes
- `ExamplePipeline_cancellation` - Graceful cancellation

## Testing

Run tests:
```bash
go test ./pipeline -v
```

Run benchmarks:
```bash
go test ./pipeline -bench=. -benchmem
```

Check coverage:
```bash
go test ./pipeline -cover
# Currently: 100% coverage
```

## Architecture

### Worker Pool
- Fixed number of goroutines (specified at creation)
- Workers pull jobs from shared input channel
- Results pushed to shared output channel
- Clean shutdown with sync.WaitGroup

### Backpressure
- Unbuffered channels by default
- Optional buffering via Options
- Producers block when workers are busy
- No unbounded queues or goroutine explosions

### Cancellation
- Context flows through entire pipeline
- Workers check ctx.Done() at two points:
  1. Before accepting new job
  2. Before sending result
- Per-job timeouts use child contexts
- Clean shutdown guaranteed

### Statistics
- Atomic counters (no channels/mutexes)
- Lock-free reads and writes
- Zero contention on happy path
- Safe for concurrent access

## Design Decisions

See README.md for detailed discussion of:
- Why bounded concurrency over dynamic scaling
- Why channels for coordination
- Why context as single source of truth
- Why guard result emission with context
- Why atomic counters for metrics

## Edge Cases Handled

- Zero workers (defaults to 1)
- Nil handler (panics at creation)
- Nil context (panics at Run/Start)
- Empty pipeline (no jobs sent)
- Closed input channel
- Context cancellation during work
- Per-job timeouts
- Handler errors
- Concurrent Stats() calls
- Pipeline reuse (multiple Run() calls)

## Thread Safety

- `New()` - safe
- `Start()` - safe, can be called multiple times
- `Run()` - safe, can be called multiple times
- `Stats()` - safe, can be called concurrently
- Pipeline struct - read-only after creation

## Comparison with Alternatives

| Feature | go-flow | errgroup | Pool patterns |
|---------|---------|----------|---------------|
| Type-safe generics | âœ… | âŒ | âŒ |
| Fixed worker count | âœ… | âŒ | âœ… |
| Result collection | âœ… | Error only | Manual |
| Per-job timeouts | âœ… | Manual | Manual |
| Statistics | âœ… | âŒ | Manual |
| Zero allocations | âœ… | âŒ | Varies |

## License

See LICENSE file in repository root.
